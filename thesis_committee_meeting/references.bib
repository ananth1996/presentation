@article{mahadevan2022certifiable,
 abstract = {Machine unlearning is the task of updating machine learning (ML) models after a subset of the training data they were trained on is deleted. Methods for the task are desired to combine effectiveness and efficiency (i.e., they should effectively “unlearn” deleted data, but in a way that does not require excessive computational effort (e.g., a full retraining) for a small amount of deletions). Such a combination is typically achieved by tolerating some amount of approximation in the unlearning. In addition, laws and regulations in the spirit of “the right to be forgotten” have given rise to requirements for certifiability (i.e., the ability to demonstrate that the deleted data has indeed been unlearned by the ML model). In this paper, we present an experimental study of the three state-of-the-art approximate unlearning methods for logistic regression and demonstrate the trade-offs between efficiency, effectiveness and certifiability offered by each method. In implementing this study, we extend some of the existing works and describe a common unlearning pipeline to compare and evaluate the unlearning methods on six real-world datasets and a variety of settings. We provide insights into the effect of the quantity and distribution of the deleted data on ML models and the performance of each unlearning method in different settings. We also propose a practical online strategy to determine when the accumulated error from approximate unlearning is large enough to warrant a full retraining of the ML model.},
 author = {Mahadevan, Ananth and Mathioudakis, Michael},
 doi = {10.3390/make4030028},
 issn = {2504-4990},
 journal = {Machine Learning and Knowledge Extraction},
 number = {3},
 pages = {591--620},
 title = {Certifiable Unlearning Pipelines for Logistic Regression: An Experimental Study},
 url = {https://www.mdpi.com/2504-4990/4/3/28},
 volume = {4},
 year = {2022}
}
@inbook{wang2023fmmd,
	author = {Yanhao Wang and Michael Mathioudakis and Jia Li and Francesco Fabbri},
	booktitle = {Proceedings of the 2023 SIAM International Conference on Data Mining (SDM)},
	doi = {10.1137/1.9781611977653.ch11},
	eprint = {https://epubs.siam.org/doi/pdf/10.1137/1.9781611977653.ch11},
	pages = {91-99},
	title = {Max-Min Diversification with Fairness Constraints: Exact and Approximation Algorithms},
	url = {https://epubs.siam.org/doi/abs/10.1137/1.9781611977653.ch11},
	bdsk-url-1 = {https://epubs.siam.org/doi/abs/10.1137/1.9781611977653.ch11},
	bdsk-url-2 = {https://doi.org/10.1137/1.9781611977653.ch11}}

@article{merchant2022JANE,
 abstract = {The task of node classification concerns a network where nodes are associated with labels, but labels are known only for some of the nodes. The task consists of inferring the unknown labels given the known node labels, the structure of the network, and other known node attributes. Common node classification approaches are based on the assumption that adjacent nodes have similar attributes and, therefore, that a node’s label can be predicted from the labels of its neighbors. While such an assumption is often valid (e.g., for political affiliation in social networks), it may not hold in some cases. In fact, nodes that share the same label may be adjacent but differ in their attributes, or may not be adjacent but have similar attributes. In this work, we present JANE (Jointly using Attributes and Node Embeddings), a novel and principled approach to node classification that flexibly adapts to a range of settings wherein unknown labels may be predicted from known labels of adjacent nodes in the network, other node attributes, or both. Our experiments on synthetic data highlight the limitations of benchmark algorithms and the versatility of JANE. Further, our experiments on seven real datasets of sizes ranging from 2.5K to 1.5M nodes and edge homophily ranging from 0.86 to 0.29 show that JANE scales well to large networks while also demonstrating an up to 20% improvement in accuracy compared to strong baseline algorithms.},
 article-number = {906},
 author = {Merchant, Arpit and Mahadevan, Ananth and Mathioudakis, Michael},
 doi = {10.3390/e24070906},
 issn = {1099-4300},
 journal = {Entropy},
 number = {7},
 pubmedid = {35885129},
 title = {Scalably Using Node Attributes and Graph Structure for Node Classification},
 url = {https://www.mdpi.com/1099-4300/24/7/906},
 volume = {24},
 year = {2022}
}
@inproceedings{10.1145/3511808.3557687,
author = {Mahadevan, Ananth and Merchant, Arpit and Wang, Yanhao and Mathioudakis, Michael},
title = {Robustness of Sketched Linear Classifiers to Adversarial Attacks},
year = {2022},
isbn = {9781450392365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3511808.3557687},
doi = {10.1145/3511808.3557687},
abstract = {Linear classifiers are well-known to be vulnerable to adversarial attacks: they may predict incorrect labels for input data that are adversarially modified with small perturbations. However, this phenomenon has not been properly understood in the context of sketch-based linear classifiers, typically used in memory-constrained paradigms, which rely on random projections of the features for model compression. In this paper, we propose novel Fast-Gradient-Sign Method (FGSM) attacks for sketched classifiers in full, partial, and black-box information settings with regards to their internal parameters. We perform extensive experiments on the MNIST dataset to characterize their robustness as a function of perturbation budget. Our results suggest that, in the full-information setting, these classifiers are less accurate on unaltered input than their uncompressed counterparts but just as susceptible to adversarial attacks. But in more realistic partial and black-box information settings, sketching improves robustness while having lower memory footprint.},
booktitle = {Proceedings of the 31st ACM International Conference on Information \& Knowledge Management},
pages = {4319-4323},
numpages = {5},
keywords = {adversarial machine learning, robustness, sketching},
location = {Atlanta, GA, USA},
series = {CIKM '22}
}

@article{Rosson-2023,
 abstract = {The Reception Reader is a web tool for studying text reuse in the Early English Books Online (EEBO-TCP) and Eighteenth Century Collections Online (ECCO) data. Users can: 1) explore a visual overview of the reception of a work, or its incoming connections, across time based on shared text segments, 2) interactively survey the details of connected documents, and 3) examine the context of reused text for \“close reading\”. We show examples of how the tool streamlines research and exploration tasks, and discuss the utility and limitations of the user interface along with its current data sources.},
 author = {Rosson, David and Mäkelä, Eetu and Vaara, Ville and Mahadevan, Ananth and Ryan, Yann and Tolonen, Mikko},
 doi = {10.5334/johd.101},
 journal = {Journal of Open Humanities Data},
 keyword = {en_US},
 month = {Apr},
 title = {Reception Reader: Exploring Text Reuse in Early Modern British Publications},
 year = {2023}
}
@misc{mahadevan2023costeffective,
    title={Cost-Effective Retraining of Machine Learning Models},
    author={Ananth Mahadevan and Michael Mathioudakis},
    year={2023},
    eprint={2310.04216},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}
@inproceedings{merchant2022JANEorig,
	address = {Cham},
	author = {Merchant, Arpit and Mathioudakis, Michael},
	booktitle = {Complex Networks {\&} Their Applications X},
	editor = {Benito, Rosa Maria and Cherifi, Chantal and Cherifi, Hocine and Moro, Esteban and Rocha, Luis M. and Sales-Pardo, Marta},
	pages = {511--522},
	publisher = {Springer International Publishing},
	title = {Joint Use of Node Attributes and Proximity for Node Classification},
	year = {2022}}

@inproceedings{tai2018sketch,
author = {Tai, Kai Sheng and Sharan, Vatsal and Bailis, Peter and Valiant, Gregory},
title = {Sketching Linear Classifiers over Data Streams},
year = {2018},
isbn = {9781450347037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3183713.3196930},
doi = {10.1145/3183713.3196930},
abstract = {We introduce a new sub-linear space sketch---the Weight-Median Sketch---for learning compressed linear classifiers over data streams while supporting the efficient recovery of large-magnitude weights in the model. This enables memory-limited execution of several statistical analyses over streams, including online feature selection, streaming data explanation, relative deltoid detection, and streaming estimation of pointwise mutual information. Unlike related sketches that capture the most frequently-occurring features (or items) in a data stream, the Weight-Median Sketch captures the features that are most discriminative of one stream (or class) compared to another. The Weight-Median Sketch adopts the core data structure used in the Count-Sketch, but, instead of sketching counts, it captures sketched gradient updates to the model parameters. We provide a theoretical analysis that establishes recovery guarantees for batch and online learning, and demonstrate empirical improvements in memory-accuracy trade-offs over alternative memory-budgeted methods, including count-based sketches and feature hashing.},
booktitle = {Proceedings of the 2018 International Conference on Management of Data},
pages = {757–772},
numpages = {16},
keywords = {sketching, online learning, linear classification},
location = {Houston, TX, USA},
series = {SIGMOD '18}
}