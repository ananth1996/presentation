\documentclass[pdf]{beamer}
\usetheme{Madrid}
\input{macros.tex}
\usepackage{multicol, latexsym, amsmath, amssymb}
\usepackage{appendixnumberbeamer} 
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{booktabs}
\usepackage{multicol}
\usetikzlibrary{arrows.meta}
\usetikzlibrary{shapes,arrows}
\usetikzlibrary{calc}    %coordinate calculation
\usepackage{xr}
\usepackage[linesnumbered,ruled]{algorithm2e}

\setbeamertemplate{navigation symbols}{}


\AtBeginSection[]{
  \begin{frame}
  \vfill
  \centering
  \begin{beamercolorbox}[sep=8pt,center,shadow=true,rounded=true]{title}
    \usebeamerfont{title}\insertsectionhead\par%
  \end{beamercolorbox}
  \vfill
  \end{frame}
}

\AtBeginSubsection[]{
  \begin{frame}
  \vfill
  \centering
  \begin{beamercolorbox}[sep=8pt,center,shadow=true,rounded=true]{title}
    \usebeamerfont{title}\insertsubsectionhead\par%
  \end{beamercolorbox}
  \vfill
  \end{frame}
}



\usepackage{subcaption}

\title[DeltaGrad]{Weekly Presentation\\DeltaGrad: Rapid retraining of machine learning models}

\author[Wu et al.~]{Yinjun Wu \and Edgar Dobriban \and Susan B Davidson\\ Presented by : Ananth Mahadevan}
\date{\today}

\begin{document}
\begin{frame}
    \titlepage
\end{frame}

\begin{frame}
    \frametitle{Overview}
    \tableofcontents
\end{frame}


\section{Motivation}
\begin{frame}
    \frametitle{Retaining Problem}
    Regular Machine Learning Pipeline:
    \begin{enumerate}
      \item Train a ML model from data using a learning algorithm
      \item Small change in training data occurs (deletions or additions)
      \item Retrain ML model from scratch
    \end{enumerate}
    \uncover<2->{Limitations:}
    \begin{itemize}
      \item<3-> Computationally expensive process
      \item<4-> Throws away useful computations from initial training
    \end{itemize}
    \begin{block}<5->{Research Question}
      Can we retrain models in an efficient manner?
    \end{block}
\end{frame}

\begin{frame}
  \frametitle{Potential Applications}
  \begin{itemize}
    \item<1-> \textbf{GDPR}: Deletion of private information from public datasets
    \item<2-> \textbf{Continuous Model Updating}: Handle additions, deletions and changes of training samples
    \item<3-> \textbf{Data Valuation}: \textit{Leave One Out} tests to find important training samples 
    \item<4-> \textbf{Bias Reduction}: Speeds up jackknife resampling that requires retrained model parameters 
  \end{itemize}
  

\end{frame}
\section{Related Work}

\begin{frame}
  \frametitle{Prior Work}
  \begin{itemize}
    \item Prior work for specialized problems and ML models, usually for deletion
    \begin{itemize}
      \item Provenane Based deletions for linear and logistic regression \cite{wuPrIUProvenanceBasedApproach2020}
      \item Newton step and noise for \textit{certified data removal} \cite{guoCertifiedDataRemoval2020}
      \item K-means clustering \cite{ginartMakingAIForget2019}
    \end{itemize}
  \end{itemize}
\end{frame}

\section{DeltaGrad}
\begin{frame}
  \frametitle{Gradient Descent}
  \begin{itemize}
    \item<1-> Objective function, 
    \[
        F(\w) = \frac{1}{n}\sum_{i=1}^{n}F_{i}(\w),
    \]
    where $F_{i}(\w)$ is loss for $i$-th sample.
    \item<2-> Stochastic Gradient Descent update rule, $\mathcal{B}_{t}$ is randomly sampled mini-batch of size $B$
    \[
      \w_{t+1} \leftarrow \w_{t}-\frac{\eta_{t}}{B} \sum_{i \in \mathcal{B}_{t}} \nabla F_{i}\left(\w_{t}\right) 
    \]
    \item<3-> Full-batch gradient descent (GD) is on entire data 
    \[
      \w_{t+1} \leftarrow \w_{t}-\frac{\eta_{t}}{n} \sum_{i=1}^{n} \nabla F_{i}\left(\w_{t}\right) 
    \]
  \end{itemize} 
\end{frame}

\begin{frame}
  \frametitle{Removal of data}
  \begin{itemize}
    \item<1-> After training,  $R = \{i_1,i_2,\dots,i_r\}$ samples are removed, where $r \ll n$
    \item<2-> Naive retraining is applying GD over remaining samples, $\w^{U}$ is the resulting model parameter
    \begin{align}\label{eq: update_rule_naive} 
      & \uw_{t+1} \leftarrow \uw_{t} - \frac{\eta_t}{n-r}\sum_{\substack{i \not\in R}} \nabla F_i\left(\uw_{t}\right) 
    \end{align}
    \item<3-> The explicit gradient computation $\sum_{\substack{i \not\in R}} \nabla F_i\left(\uw_{t}\right)$ is expensive
    \item<4-> Instead rewrite \eqref{eq: update_rule_naive} as the ``\textit{leave-r-out}'' formula
    \begin{align}\label{eq: approx_w_t}
      \begin{split}
      \uw_{t+1} 
               = \uw_{t} - \frac{\eta_t}{n-r}\left[\sum_{i=1}^{n} \nabla F_{i}\left(\uw_{t}\right) - \sum_{\substack{i \in R}} \nabla F_i\left(\uw_{t}\right)\right].
      \end{split}
      \end{align}
      \item<5-> $\sum_{\substack{i \in R}} \nabla F_i\left(\uw_{t}\right)$ is cheaper to compute
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Etymology}
  \begin{itemize}
    \item After a small change to the data we need to redo the SGD computations
    \item We can achieve this by understanding the \textit{delta} of the Gradient Descent 
    \[ 
      n\nabla F(\w) = \sum_{i=1}^{n}\nabla F_{i}(\w_t) \quad \& \quad n\nabla F(\uw) =  \sum_{i=1}^{n} \nabla F_{i}\left(\uw_{t}\right)
    \]
    \item Hence, the approach is called \textit{DeltaGrad}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Aprroximating $\nabla F(\uw)$}
  \begin{itemize}
    \item<1-> $\w_{0}$, $\dots,\w_{t}$ and $\nabla F\left(\w_{0}\right)$, $\dots, \nabla F\left(\w_{t}\right)$ are cached from training on initial dataset
    \item<2-> By Cauchy mean-value theorem\footnote<3->{Actually a consequence of Fundamental theory of Calculus and mean-value theorem}
    \[
      \nabla F(\uw_{t}) - \nabla F(\w_{t}) = \bH_{t} \cdot (\uw_{t} - \w_{t}) 
    \]
    Where $\bH_{t} = \int_{0}^{1}\bH(\w_{t}+x(\uw_{t}-\w_{t}))dx$ is the integrated hessian
    \item<4-> This requires a hessian $\bH_{t}$ at each step, which is expensive to maintain and evaluate
    \item<5-> Leverage classical L-BFGS algorithm to approximate $\bH_{t}$
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Review of L-BFGS}
  \begin{itemize}
    \item Traditional L-BFGS updates gradients using
    \[\nabla F(\w_{t+1}) - \nabla F(\w_{t})  = \B_{t} \cdot (\w_{t+1}-\w_{t}) \]
    Where, $\B_t$ is the approximation of the hessian
  \end{itemize}
  \vfill
  \uncover<2->{\scalebox{0.8}{\begin{minipage}{0.55\textwidth}
    Traditional L-BFGS
          \[
  \begin{array}{l}
  \nabla F\left(\w_{t+1}\right)-\nabla F\left(\w_{t}\right) \approx \mathbf{B}_{t}\left(\w_{t+1}-\w_{t}\right) \\
  \mathbf{B}_{t} \approx \mathbf{H}_{t} \\
  =\int_{0}^{1} \mathbf{H}\left(\w_{t}+x\left(\w_{t+1}-\w_{t}\right)\right) d x \\
  \s_{t}=\w_{t+1}-\w_{t} \\
  \y_{t}=\nabla F\left(\w_{t+1}\right)-\nabla F\left(\w_{t}\right)
  \end{array}
    \]
  \end{minipage}
  }
  }
  \uncover<3->{
  \hfill%
  \scalebox{0.8}{
  \begin{minipage}{0.55\textwidth}
    L-BFGS for approximating $\nabla F(\uw)$
          \[
  \begin{array}{l}
  \nabla F\left(\uw_{t}\right)-\nabla F\left(\w_{t}\right) \approx \mathbf{B}_{t}\left(\uw_{t}-\w_{t}\right) \\
  \mathbf{B}_{t} \approx \mathbf{H}_{t} \\
  =\int_{0}^{1} \mathbf{H}\left(\w_{t}+x\left(\uw_{t}-\w_{t}\right)\right) d x \\
  \s_{t}=\uw_{t}-\w_{t} \\
  \y_{t}=\nabla F\left(\uw_{t}\right)-\nabla F\left(\w_{t}\right)
\end{array}
  \]
    \end{minipage}
    }
  }
\end{frame}

\begin{frame}
  \frametitle{Using L-BFGS}
  \begin{itemize}
    \item Maintain $m$ historical observations of $\textbf{Y} = (\y_{t},\y_{t-1},\dots,\y_{t-m})$ and $\textbf{S} = (\s_{t},\s_{t-1},\dots,\s_{t-m})$
    \item Let $g$ be a function defined by L-BFGS, then we can approximate $\B_{t}\cdot\textbf{v}$ using 
    \[
        g(\textbf{Y},\textbf{S},\textbf{v}) 
    \]
    Where, $\textbf{v}$ is an arbitrary vector.
    \item Therefore,  
    \[ \B_{t}\cdot(\uw_{t}-\w_{t}) = g(\textbf{Y},\textbf{S},\uw_{t}-\w_{t}) \]
    \item Hence we obtain the approximation as 
    \[
        \nabla F(\uw_{t}) \approx \nabla F(\w_{t}) + \B_{t} \cdot (\uw_{t}-\w_{t}) 
    \]
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Rewriting}
  \begin{itemize}
    \item Denoting $\iw$ as the approximate $\uw$ we have 
    \[
      \nabla F(\iw_{t}) \approx \nabla F(\w_{t}) + \B_{t} \cdot (\iw_{t}-\w_{t}).
    \]
    \item replacing in \eqref{eq: approx_w_t}
    \begin{align*}
      \iw_{t+1} 
      &= \iw_{t} - \frac{\eta_t}{n-r}\left[\sum_{i=1}^{n} \nabla F_{i}\left(\iw_{t}\right) - \sum_{\substack{i \in R}} \nabla F_i\left(\iw_{t}\right)\right] \\
      &= \iw_{t} - \frac{\eta_{t}}{n-r} \left\{ n[\B_{t}(\iw_t - \w_t) + \nabla F(\w_t)] - \sum_{i\in R} \nabla F(\iw_t)\right\}
    \end{align*}
  \end{itemize}
\end{frame}


\begin{frame}
  \frametitle{Problem with Error Bound}
  \includegraphics<2->[page=43,clip,trim=0.5cm 1cm 0cm 1cm,width=\textwidth]{images/Slides.pdf}
\end{frame}

\begin{frame}
  \frametitle{Controlling the Errors}
  \begin{itemize}
    \item Do explicit evaluations for $j_{0}$ "burn-in" iterations and then periodically every $T_{0}$ iterations
    \includegraphics[page=46,clip,trim=0.5cm 1cm 0cm 1cm,width=0.9\textwidth]{images/Slides.pdf}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Benefit of DeltaGrad}
  \begin{itemize}
    \item DeltaGrad can be extended to when $r$ samples are added rather than deleted
    \item Change the $+$ to minus in the update formula to get 
    \[
      \iw_{t+1}  = \iw_{t} - \frac{\eta_{t}}{n\textcolor{red}{+}r} \left\{ n[\B_{t}(\iw_t - \w_t) + \nabla F(\w_t)] \textcolor{red}{+} \sum_{i\in R} \nabla F(\iw_t)\right\}
    \]
    \item Here $\sum_{i\in R} \nabla F(\iw_t)$ is the gradient of the added $r$ samples
  \end{itemize}
  
\end{frame}

\begin{frame}
  \frametitle{Algorithm}
  \begin{center}
    
    \scalebox{.6}{ 
      \begin{algorithm}[H]
        \small
        \SetKwInOut{Input}{Input}
        \SetKwInOut{Output}{Output}
        \Input{The full training set $\left(\textbf{X}, \textbf{Y}\right)$, model parameters cached during the training phase over the full training samples $\{\w_{0}, \w_{1}, \dots, \w_{t}\}$ and corresponding gradients $\{\nabla F\left(\w_{0}\right), \nabla F\left(\w_{1}\right), \dots, \nabla F\left(\w_{t}\right)\}$, the indices of the removed training samples $R$, period $T_0$, total iteration number $T$, history size $m$, ``burn-in'' iteration number $j_0$, learning rate $\eta_t$}
        \Output{Updated model parameter $\iw_{t}$}
        Initialize $\iw_{0} \leftarrow \w_{0}$
        
        Initialize an array $\Delta G = \left[\right]$
        
        Initialize an array $\Delta W = \left[\right]$
        
        \For{$t=0;t<T; t++$}{
          
          \eIf{$[((t-j_0) \mod T_0) == 0]$ or $t \leq j_0$}
          {
            compute $\nabla F\left(\iw_{t}\right)$ exactly
            
            compute $\nabla F\left(\iw_{t}\right) - \nabla F\left(\w_{t}\right)$ based on the cached gradient $\nabla F\left(\w_{t}\right)$
            
            set $\Delta G\left[k\right] = \nabla F\left(\iw_{t}\right) - \nabla F\left(\w_{t}\right)$
            
            set $\Delta W\left[k\right] = \iw_{t} - \w_{t}$, based on the cached parameters $\w_{t}$
            
            $k\leftarrow k+1$
            
            compute $\iw_{t+1}$ by using exact GD update (equation \eqref{eq: update_rule_naive})
            }
            {
              Pass $\Delta W\left[-m:\right]$, $\Delta G\left[-m:\right]$, the last $m$ elements in $\Delta W$ and $\Delta G$, which are from the $j_1^{th}, j_2^{th},\dots, j_m^{th}$ iterations where $j_1 < j_2< \dots < j_m$ depend on $t$, $\textbf{v} = \iw_{t} - \w_{t}$, and the history size $m$, to the L-BFGFS Algorithm to get the approximation of $\bH(\w_{t})\textbf{v}$, i.e., $\B_{j_m}\textbf{v}$
              
              Approximate $\nabla F\left(\iw_{t}\right) = \nabla F\left(\w_{t}\right) + \B_{j_m}\left(\iw_{t} - \w_{t}\right)$
              
              Compute $\iw_{t+1}$ by using the "leave-$r$-out" gradient formula, based on the approximated $\nabla F(\iw_{t})$ 
              }
              }
              
              \Return $\iw_{t}$
              \caption{DeltaGrad}
              \label{alg: update_algorithm}
            \end{algorithm}
            }
          \end{center}

\end{frame}
\section{Theoretical Results}

\begin{frame}
  \frametitle{Results for GD}\
  \label{theoretical}
  \begin{block}{Theorem (Bound between true and incrementally updated iterates)}
    Assuming $F(\w)$ is strongly convex, for large enough iterations $t$, the result $\iw_{t}$ of \textit{DeltaGrad} approximates the correct iteration values $\uw_{t}$ at the rate of 
    $$
    \|\uw_{t}-\iw_{t}\| = o\left( \frac{r}{n} \right)
    $$
  So $\|\uw_{t}-\iw_{t}\|$ is of a lower order than $r/n$.
  $r/n$ is the "baseline error rate" of the original weights $\w_{t}$, i.e., $\|\w_{t}-\iw_{t}\| = o(\frac{r}{n})$ 
  \end{block}
  \hyperlink{architecture}{\beamergotobutton{architecture of proof}}
\end{frame}

\begin{frame}
  \frametitle{Results for SGD}
  \begin{block}{Theorem(Bound between true and incrementally updated iterates in SGD)}
  Assuming $F(\w)$ is strongly convex, for large enough iterations $t$ and mini-batch size $B$, the result $\iw_{t}$ of \textit{DeltaGrad} approximates the correct iteration values $\uw_{t}$ at the rate of
  $$
  \|\uw_{t}-\iw_{t}\| = o\left( \frac{r}{n} +\frac{1}{B^{\frac{1}{4}}}\right)
  $$
  with high probability
  \end{block}
  

\end{frame}
\section{Experimental Results}
\begin{frame}
  \frametitle{Setup}
  \begin{itemize}
    \item {\bf Datasets}: MNIST, RCV1, HIGGS
    \item {\bf Model}: Logistic regression with L2 regularization
    \item {\bf Baseline}: Naive retraining (BaseL)
    \item {\bf Hyperparameters}: $j_0 = \{10,10,300\}$ and $T_0 = \{5,10,3\}$
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Results}
  \label{results}
  \input{images/results.tex}
  \hyperlink{batch-performance}{\beamergotobutton{Batch Performance}}
  \hyperlink{Online-Performance}{\beamergotobutton{Online Performance}}
  \hyperlink{large-deletions}{\beamergotobutton{Large Deletions}}
  \hyperlink{resnet-results}{\beamergotobutton{ResNet512 Results}}
\end{frame}


\section{Future Work}
\begin{frame}
  \frametitle{Our Research Directions}
  \begin{itemize}
    \item {\bf What can we forget?} \\
    Selectively cache $\w_{t}$ and $\nabla F(\w_{t})$ during original training, and still uphold the update approximation guarantee
    \item {\bf How to perform consecutive updates?}\\
    Are there issues with cumulative approximations? Compare online machine learning with deletions to DeltaGrad.
    \item {\bf When should one retrain?}
    After how many additions/deletions does $\w_{t}$ and $\uw_{t}$ diverge beyond approximation guarantees? Can a complete retraining benefit from prior updates performed?
  \end{itemize}
  
  
\end{frame}

\begin{frame}[allowframebreaks]
  \frametitle{References}
  \bibliography{UpdateMl}
  \bibliographystyle{alpha}
\end{frame}

\appendix
\section{Additional Results}
\label{additional}
\begin{frame}
  \frametitle{Batch Performance}
  \label{batch-performance}
  \begin{center}
    \includegraphics[width=0.7\textwidth]{images/Batch results.png}
  \end{center}
\end{frame}

\begin{frame}
  \frametitle{Online Performance}
  \label{Online-Performance}
  \begin{center}
    \includegraphics[width=\textwidth]{images/Online Results.png}
  \end{center}
\end{frame}

\begin{frame}
  \frametitle{MNIST Deletions upto 20\%}
  \label{large-deletions}
  \begin{center}
    \includegraphics[width=\textwidth]{images/large deletions.png}
  \end{center}
\end{frame}
\begin{frame}
  \frametitle{ResNet512 Results}
  \label{resnet-results}
  \begin{center}
    \includegraphics[width=\textwidth]{images/resnet results.png}
  \end{center}
\end{frame}

\section{Proof Architecture}
\begin{frame}
  \label{architecture}
  \frametitle{Reursive Architecture of Proof}
  \begin{center}
  \scalebox{0.59}{
    \input{images/architexture.tex}
  }
  \hyperlink{theoretical}{\beamerreturnbutton{Theoretical Results}}
  \end{center}  
\end{frame}


\end{document}  