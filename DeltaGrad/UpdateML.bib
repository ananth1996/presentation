
@article{brophyDARTDataAddition2020,
  title = {{{DART}}: {{Data Addition}} and {{Removal Trees}}},
  shorttitle = {{{DART}}},
  author = {Brophy, Jonathan and Lowd, Daniel},
  year = {2020},
  month = sep,
  abstract = {How can we update data for a machine learning model after it has already trained on that data? In this paper, we introduce DART, a variant of random forests that supports adding and removing training data with minimal retraining. Data updates in DART are exact, meaning that adding or removing examples from a DART model yields exactly the same model as retraining from scratch on updated data. DART uses two techniques to make updates efficient. The first is to cache data statistics at each node and training data at each leaf, so that only the necessary subtrees are retrained. The second is to choose the split variable randomly at the upper levels of each tree, so that the choice is completely independent of the data and never needs to change. At the lower levels, split variables are chosen to greedily maximize a split criterion such as Gini index or mutual information. By adjusting the number of random-split levels, DART can trade off between more accurate predictions and more efficient updates. In experiments on ten real-world datasets and one synthetic dataset, we find that DART is orders of magnitude faster than retraining from scratch while sacrificing very little in terms of predictive performance.},
  archivePrefix = {arXiv},
  eprint = {2009.05567},
  eprinttype = {arxiv},
  file = {/Users/mahadeva/OneDrive - University of Helsinki/ZoteroAttachments/UpdateML/DART/Brophy_Lowd_2020_DART.pdf},
  journal = {arXiv:2009.05567 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@incollection{ginartMakingAIForget2019,
  title = {Making {{AI Forget You}}: {{Data Deletion}} in {{Machine Learning}}},
  shorttitle = {Making {{AI Forget You}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 32},
  author = {Ginart, Antonio and Guan, Melody and Valiant, Gregory and Zou, James Y},
  editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and d{\textbackslash}textquotesingle {Alch{\'e}-Buc}, F. and Fox, E. and Garnett, R.},
  year = {2019},
  pages = {3518--3531},
  publisher = {{Curran Associates, Inc.}},
  file = {/Users/mahadeva/OneDrive - University of Helsinki/ZoteroAttachments/UpdateML/Making AI Forget You/Ginart et al_2019_Making AI Forget You.pdf;/Users/mahadeva/Zotero/storage/QQ32KP9R/Ginart et al. - 2019 - Making AI Forget You Data Deletion in Machine Lea.html}
}

@article{guoCertifiedDataRemoval2020,
  title = {Certified {{Data Removal}} from {{Machine Learning Models}}},
  author = {Guo, Chuan and Goldstein, Tom and Hannun, Awni and {van der Maaten}, Laurens},
  year = {2020},
  month = aug,
  abstract = {Good data stewardship requires removal of data at the request of the data's owner. This raises the question if and how a trained machine-learning model, which implicitly stores information about its training data, should be affected by such a removal request. Is it possible to "remove" data from a machine-learning model? We study this problem by defining certified removal: a very strong theoretical guarantee that a model from which data is removed cannot be distinguished from a model that never observed the data to begin with. We develop a certified-removal mechanism for linear classifiers and empirically study learning settings in which this mechanism is practical.},
  archivePrefix = {arXiv},
  eprint = {1911.03030},
  eprinttype = {arxiv},
  file = {/Users/mahadeva/OneDrive - University of Helsinki/ZoteroAttachments/UpdateML/Certified Data Removal from Machine Learning Models/Guo et al_2020_Certified Data Removal from Machine Learning Models.pdf},
  journal = {arXiv:1911.03030 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{schelterAmnesiaMachineLearning,
  title = {``{{Amnesia}}'' \textendash{} {{Towards Machine Learning Models That Can Forget User Data Very Fast}}},
  author = {Schelter, Sebastian},
  pages = {4},
  abstract = {Software systems that learn from user data with machine learning (ML) techniques have become ubiquitous over the last years. Recent law requires companies and institutions that process personal data to delete user data upon request (enacting the ``right to be forgotten''). However, it is not sufficient to merely delete the user data from databases. ML models that have been learnt from the stored data can be considered a lossy compressed version of the data, and therefore it can be argued that the user data must also be removed from them. Typically, this requires an inefficient and costly retraining of the affected ML models from scratch, as well as access to the original training data.},
  file = {/Users/mahadeva/Zotero/storage/EB984AJV/Schelter - “Amnesia” – Towards Machine Learning Models That C.pdf},
  language = {en}
}

@article{wuDeltaGradRapidRetraining2020,
  title = {{{DeltaGrad}}: {{Rapid}} Retraining of Machine Learning Models},
  shorttitle = {{{DeltaGrad}}},
  author = {Wu, Yinjun and Dobriban, Edgar and Davidson, Susan B.},
  year = {2020},
  month = jun,
  abstract = {Machine learning models are not static and may need to be retrained on slightly changed datasets, for instance, with the addition or deletion of a set of data points. This has many applications, including privacy, robustness, bias reduction, and uncertainty quantifcation. However, it is expensive to retrain models from scratch. To address this problem, we propose the DeltaGrad algorithm for rapid retraining machine learning models based on information cached during the training phase. We provide both theoretical and empirical support for the effectiveness of DeltaGrad, and show that it compares favorably to the state of the art.},
  archivePrefix = {arXiv},
  eprint = {2006.14755},
  eprinttype = {arxiv},
  file = {/Users/mahadeva/OneDrive - University of Helsinki/ZoteroAttachments/UpdateML/DeltaGrad/Slides.pdf;/Users/mahadeva/OneDrive - University of Helsinki/ZoteroAttachments/UpdateML/DeltaGrad/Wu et al_2020_DeltaGrad_annotated.pdf;/Users/mahadeva/OneDrive - University of Helsinki/ZoteroAttachments/UpdateML/DeltaGrad/Wu et al_2020_DeltaGrad.pdf;/Users/mahadeva/Zotero/storage/RXXIBZYK/2006.html},
  journal = {arXiv:2006.14755 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@inproceedings{wuPrIUProvenanceBasedApproach2020,
  title = {{{PrIU}}: {{A Provenance}}-{{Based Approach}} for {{Incrementally Updating Regression Models}}},
  shorttitle = {{{PrIU}}},
  booktitle = {Proceedings of the 2020 {{ACM SIGMOD International Conference}} on {{Management}} of {{Data}}},
  author = {Wu, Yinjun and Tannen, Val and Davidson, Susan B.},
  year = {2020},
  month = jun,
  pages = {447--462},
  publisher = {{ACM}},
  address = {{Portland OR USA}},
  doi = {10.1145/3318464.3380571},
  abstract = {The ubiquitous use of machine learning algorithms brings new challenges to traditional database problems such as incremental view update. Much effort is being put in better understanding and debugging machine learning models, as well as in identifying and repairing errors in training datasets. Our focus is on how to assist these activities when they have to retrain the machine learning model after removing problematic training samples in cleaning or selecting different subsets of training data for interpretability. This paper presents an efficient provenance-based approach, PrIU, and its optimized version, PrIU-opt, for incrementally updating model parameters without sacrificing prediction accuracy. We prove the correctness and convergence of the incrementally updated model parameters, and validate it experimentally. Experimental results show that up to two orders of magnitude speed-ups can be achieved by PrIU-opt compared to simply retraining the model from scratch, yet obtaining highly similar models.},
  file = {/Users/mahadeva/OneDrive - University of Helsinki/ZoteroAttachments/UpdateML/PrIU/Wu et al_2020_PrIU.pdf},
  isbn = {978-1-4503-6735-6},
  language = {en}
}


